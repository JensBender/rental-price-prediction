{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb7c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853219bb",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c0c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv\n",
    "df = pd.read_csv(\"data/rental_prices_singapore_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b29510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1680 entries, 0 to 1679\n",
      "Data columns (total 18 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   price               1680 non-null   int64  \n",
      " 1   size                1680 non-null   int64  \n",
      " 2   bedrooms            1680 non-null   object \n",
      " 3   bathrooms           1680 non-null   int64  \n",
      " 4   latitude            1680 non-null   float64\n",
      " 5   longitude           1680 non-null   float64\n",
      " 6   meters_to_cbd       1680 non-null   int64  \n",
      " 7   meters_to_school    1680 non-null   int64  \n",
      " 8   restaurants_rating  1680 non-null   float64\n",
      " 9   property_type       1680 non-null   object \n",
      " 10  furnishing          1680 non-null   object \n",
      " 11  year                1680 non-null   int64  \n",
      " 12  meters_to_mrt       1680 non-null   int64  \n",
      " 13  high_floor          1680 non-null   bool   \n",
      " 14  new                 1680 non-null   bool   \n",
      " 15  renovated           1680 non-null   bool   \n",
      " 16  view                1680 non-null   bool   \n",
      " 17  penthouse           1680 non-null   bool   \n",
      "dtypes: bool(5), float64(3), int64(7), object(3)\n",
      "memory usage: 179.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show dataframe info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb7013cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>meters_to_cbd</th>\n",
       "      <th>meters_to_school</th>\n",
       "      <th>restaurants_rating</th>\n",
       "      <th>property_type</th>\n",
       "      <th>furnishing</th>\n",
       "      <th>year</th>\n",
       "      <th>meters_to_mrt</th>\n",
       "      <th>high_floor</th>\n",
       "      <th>new</th>\n",
       "      <th>renovated</th>\n",
       "      <th>view</th>\n",
       "      <th>penthouse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.312952</td>\n",
       "      <td>103.887868</td>\n",
       "      <td>6744</td>\n",
       "      <td>422</td>\n",
       "      <td>4.126316</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Fully Furnished</td>\n",
       "      <td>2013</td>\n",
       "      <td>450</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>1130</td>\n",
       "      <td>Room</td>\n",
       "      <td>1</td>\n",
       "      <td>1.328820</td>\n",
       "      <td>103.912904</td>\n",
       "      <td>14317</td>\n",
       "      <td>3422</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Fully Furnished</td>\n",
       "      <td>2013</td>\n",
       "      <td>810</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7400</td>\n",
       "      <td>3800</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.389444</td>\n",
       "      <td>103.857002</td>\n",
       "      <td>15497</td>\n",
       "      <td>568</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Fully Furnished</td>\n",
       "      <td>2013</td>\n",
       "      <td>450</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>120</td>\n",
       "      <td>Room</td>\n",
       "      <td>1</td>\n",
       "      <td>1.429261</td>\n",
       "      <td>103.828917</td>\n",
       "      <td>22825</td>\n",
       "      <td>1090</td>\n",
       "      <td>3.605263</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Fully Furnished</td>\n",
       "      <td>2013</td>\n",
       "      <td>700</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4300</td>\n",
       "      <td>689</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.297356</td>\n",
       "      <td>103.836707</td>\n",
       "      <td>3070</td>\n",
       "      <td>1262</td>\n",
       "      <td>4.120000</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Fully Furnished</td>\n",
       "      <td>2013</td>\n",
       "      <td>420</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  size bedrooms  bathrooms  latitude   longitude  meters_to_cbd  \\\n",
       "0   3000   400        1          1  1.312952  103.887868           6744   \n",
       "1   2000  1130     Room          1  1.328820  103.912904          14317   \n",
       "2   7400  3800        5          4  1.389444  103.857002          15497   \n",
       "3   1000   120     Room          1  1.429261  103.828917          22825   \n",
       "4   4300   689        1          1  1.297356  103.836707           3070   \n",
       "\n",
       "   meters_to_school  restaurants_rating property_type       furnishing  year  \\\n",
       "0               422            4.126316     Apartment  Fully Furnished  2013   \n",
       "1              3422            3.800000     Apartment  Fully Furnished  2013   \n",
       "2               568            4.710000     Apartment  Fully Furnished  2013   \n",
       "3              1090            3.605263     Apartment  Fully Furnished  2013   \n",
       "4              1262            4.120000     Apartment  Fully Furnished  2013   \n",
       "\n",
       "   meters_to_mrt  high_floor    new  renovated   view  penthouse  \n",
       "0            450       False  False      False  False      False  \n",
       "1            810       False  False      False  False      False  \n",
       "2            450       False  False      False  False      False  \n",
       "3            700       False  False      False  False      False  \n",
       "4            420       False  False      False  False      False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e9c3d",
   "metadata": {},
   "source": [
    "# Handle rare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6277565b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3         479\n",
       "2         438\n",
       "1         245\n",
       "4         207\n",
       "Room      135\n",
       "5          89\n",
       "6          39\n",
       "Studio     27\n",
       "7          14\n",
       "8           5\n",
       "9           1\n",
       "10          1\n",
       "Name: bedrooms, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show category frequencies of bedrooms\n",
    "df[\"bedrooms\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9bcbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 7, 8, 9 and 10 bedrooms into one category\n",
    "df[\"bedrooms\"]= df[\"bedrooms\"].replace({\"8\": \"7\", \"9\": \"7\", \"10\": \"7\"})\n",
    "\n",
    "# Rename this category to \"7+\"\n",
    "df[\"bedrooms\"]= df[\"bedrooms\"].replace({\"7\": \"7+\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf37ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3         479\n",
       "2         438\n",
       "1         245\n",
       "4         207\n",
       "Room      135\n",
       "5          89\n",
       "6          39\n",
       "Studio     27\n",
       "7+         21\n",
       "Name: bedrooms, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show category frequencies of bedrooms\n",
    "df[\"bedrooms\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885d9d9",
   "metadata": {},
   "source": [
    "# Train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85458230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "635eead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X features and y target\n",
    "X = df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7953e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and temporary sets (70% train, 30% temporary)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary data into validation and test sets (50% each)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a22e3d",
   "metadata": {},
   "source": [
    "Note: This accomplishes a 70% training, 15% validation and 15% test set size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29502e76",
   "metadata": {},
   "source": [
    "# Handle outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8094d",
   "metadata": {},
   "source": [
    "Note: Univariate outliers regarding rental price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99936f",
   "metadata": {},
   "source": [
    "## Mean and 3 SD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9769bd",
   "metadata": {},
   "source": [
    "Criterion for outliers: 3 standard deviations (SD) above or below the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77ec1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# Create a custom transformer class to handle outliers based on 3 SD\n",
    "class OutlierHandler3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y):\n",
    "        # Calculate y mean, standard deviation, and cutoff values \n",
    "        self.mean_ = y.mean()\n",
    "        self.sd_ = y.std()\n",
    "        self.lower_cutoff_ = self.mean_ - 3 * self.sd_\n",
    "        self.upper_cutoff_ = self.mean_ + 3 * self.sd_\n",
    "        print(f\"Lower cutoff: {round(self.lower_cutoff_)} S$/month\")\n",
    "        print(f\"Upper cutoff: {round(self.upper_cutoff_)} S$/month\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        # Apply cutoff values\n",
    "        mask = (y >= self.lower_cutoff_) & (y <= self.upper_cutoff_)\n",
    "        # Print number of outliers\n",
    "        print(f\"Rental price outliers based on 3 SD: {y.shape[0] - y[mask].shape[0]}\")\n",
    "        # Return data with outliers removed \n",
    "        return X[mask], y[mask]\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "064440f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an OutlierHandler3SD object\n",
    "outlier_handler_3sd = OutlierHandler3SD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39074255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower cutoff: -37337 S$/month\n",
      "Upper cutoff: 56094 S$/month\n",
      "Rental price outliers based on 3 SD: 29\n",
      "Rental price outliers based on 3 SD: 2\n",
      "Rental price outliers based on 3 SD: 2\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers in training, validation and test data\n",
    "X_train_no_outliers, y_train_no_outliers = outlier_handler_3sd.fit_transform(X_train, y_train)\n",
    "X_val_no_outliers, y_val_no_outliers = outlier_handler_3sd.transform(X_val, y_val)\n",
    "X_test_no_outliers, y_test_no_outliers = outlier_handler_3sd.transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ef0e8",
   "metadata": {},
   "source": [
    "## Quartiles and 1.5 IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada2ae7",
   "metadata": {},
   "source": [
    "Criterion for outliers: 1.5 interquartile ranges (IQR) above the 3. quartile or below the 1. quartile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c22b77e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# Create a custom transformer class to handle outliers based on 1.5 IQR\n",
    "class OutlierHandlerIQR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y):\n",
    "        # Calculate y quartiles, IQR and cutoff values  \n",
    "        Q1 = y.quantile(0.25)\n",
    "        Q3 = y.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        self.lower_cutoff_ = Q1 - 1.5 * IQR\n",
    "        self.upper_cutoff_ = Q3 + 1.5 * IQR\n",
    "        print(f\"Lower cutoff: {round(self.lower_cutoff_)} S$/month\")\n",
    "        print(f\"Upper cutoff: {round(self.upper_cutoff_)} S$/month\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        # Apply cutoff values \n",
    "        mask = (y >= self.lower_cutoff_) & (y <= self.upper_cutoff_)\n",
    "        # Print number of outliers\n",
    "        print(f\"Rental price outliers based on 1.5 IQR: {y.shape[0] - y[mask].shape[0]}\")\n",
    "        # Return data with outliers removed \n",
    "        return X[mask], y[mask]\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        # Perform both fit and transform\n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "094f766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an OutlierHandlerIQR object\n",
    "outlier_handler_iqr = OutlierHandlerIQR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f17905b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower cutoff: -3000 S$/month\n",
      "Upper cutoff: 15400 S$/month\n",
      "Rental price outliers based on 1.5 IQR: 143\n",
      "Rental price outliers based on 1.5 IQR: 35\n",
      "Rental price outliers based on 1.5 IQR: 28\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers in training, validation and test data\n",
    "X_train_no_outliers, y_train_no_outliers = outlier_handler_iqr.fit_transform(X_train, y_train)\n",
    "X_val_no_outliers, y_val_no_outliers = outlier_handler_iqr.transform(X_val, y_val)\n",
    "X_test_no_outliers, y_test_no_outliers = outlier_handler_iqr.transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0608d2e",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d81b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184daf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store numerical columns\n",
    "numerical_columns = [\"size\", \"bathrooms\", \"latitude\", \"longitude\", \"meters_to_mrt\", \"meters_to_cbd\", \n",
    "                     \"meters_to_school\", \"restaurants_rating\", \"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a StandardScaler object (z-score normalization)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical columns\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_val_scaled = scaler.transform(X_val[numerical_columns])\n",
    "X_test_scaled = scaler.transform(X_test[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0205108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to pandas dataframes and assign the original column names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=numerical_columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=numerical_columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ff450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the index of the new dataframes to match the originial dataframes' index\n",
    "X_train_scaled.index = X_train.index\n",
    "X_val_scaled.index = X_val.index\n",
    "X_test_scaled.index = X_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a9693",
   "metadata": {},
   "source": [
    "# Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c105979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54713aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store categorical columns\n",
    "categorical_columns = [\"bedrooms\", \"property_type\", \"furnishing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a OneHotEncoder object\n",
    "encoder = OneHotEncoder(drop=None, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding of categorical columns\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_val_encoded = encoder.transform(X_val[categorical_columns])\n",
    "X_test_encoded = encoder.transform(X_test[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of the one-hot encoded features\n",
    "encoded_feature_names = encoder.get_feature_names_out(input_features=categorical_columns)\n",
    "\n",
    "# Define a mapping dictionary to change names\n",
    "column_name_mapping = {\n",
    "    \"bedrooms_Room\": \"bedrooms_room\",\n",
    "    \"bedrooms_Studio\": \"bedrooms_studio\",\n",
    "    \"property_type_Apartment\": \"type_apartment\",\n",
    "    \"property_type_Cluster House\": \"type_cluster_house\",\n",
    "    \"property_type_Condominium\": \"type_condominium\",\n",
    "    \"property_type_Corner Terrace\": \"type_corner_terrace\",\n",
    "    \"property_type_Detached House\": \"type_detached_house\",\n",
    "    \"property_type_Good Class Bungalow\": \"type_good_class_bungalow\",\n",
    "    \"property_type_HDB Flat\": \"type_hdb_flat\",\n",
    "    \"property_type_Semi-Detached House\": \"type_semi_detached_house\",\n",
    "    \"property_type_Terraced House\": \"type_terraced_house\",\n",
    "    \"furnishing_Fully Furnished\": \"furnishing_full\",\n",
    "    \"furnishing_Partially Furnished\": \"furnishing_partial\",\n",
    "    \"furnishing_Unfurnished\": \"furnishing_none\",\n",
    "}\n",
    "\n",
    "# Rename one-hot encoded features \n",
    "encoded_feature_names = np.vectorize(lambda x: column_name_mapping.get(x, x))(encoded_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb507cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to pandas dataframes and assign the column names\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoded_feature_names)\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded, columns=encoded_feature_names)\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoded_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the index of the new dataframes to match the originial dataframes' index\n",
    "X_train_encoded.index = X_train.index\n",
    "X_val_encoded.index = X_val.index\n",
    "X_test_encoded.index = X_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8972d8f",
   "metadata": {},
   "source": [
    "# Create final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45922ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store boolean columns\n",
    "boolean_columns = [\"high_floor\", \"new\", \"renovated\", \"view\", \"penthouse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f11cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine scaled numerical features, encoded categorical features, and boolean features\n",
    "X_train_final = pd.concat([X_train_scaled, X_train_encoded, X_train[boolean_columns]], axis=1)\n",
    "X_val_final = pd.concat([X_val_scaled, X_val_encoded, X_val[boolean_columns]], axis=1)\n",
    "X_test_final = pd.concat([X_test_scaled, X_test_encoded, X_test[boolean_columns]], axis=1)\n",
    "\n",
    "# Convert the boolean features from object back to boolean\n",
    "X_train_final[boolean_columns] = X_train_final[boolean_columns].astype(bool)\n",
    "X_val_final[boolean_columns] = X_val_final[boolean_columns].astype(bool)\n",
    "X_test_final[boolean_columns] = X_test_final[boolean_columns].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b6234",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32c46587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower cutoff: -37337 S$/month\n",
      "Upper cutoff: 56094 S$/month\n",
      "Rental price outliers based on 3 SD: 29\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1147,17) into shape (1147,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6472\\3913903610.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Fit and transform the entire pipeline on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mX_train_preprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Transform the validation and test data using the same pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;31m# set n_features_in_ attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m_check_X\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__array__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 820\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (1147,17) into shape (1147,)"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define numerical, categorical and boolean columns \n",
    "numerical_columns = [\"size\", \"bathrooms\", \"latitude\", \"longitude\", \"meters_to_mrt\", \"meters_to_cbd\", \n",
    "                     \"meters_to_school\", \"restaurants_rating\", \"year\"]\n",
    "categorical_columns = [\"bedrooms\", \"property_type\", \"furnishing\"]\n",
    "boolean_columns = [\"high_floor\", \"new\", \"renovated\", \"view\", \"penthouse\"]\n",
    "\n",
    "# Create transformer for numerical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Create transformer for categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"encoder\", OneHotEncoder(drop=None, sparse=False))\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numerical\", numerical_transformer, numerical_columns),\n",
    "        (\"categorical\", categorical_transformer, categorical_columns)\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Include the boolean columns without transformation\n",
    ")\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    (\"outlier_handler\", OutlierHandler3SD()),\n",
    "    (\"preprocessor\", preprocessor)\n",
    "])\n",
    "\n",
    "# Fit and transform the entire pipeline on the training data\n",
    "X_train_preprocessed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the validation and test data using the same pipeline\n",
    "# X_val_preprocessed = preprocessing_pipeline.transform(X_val, y_val)\n",
    "# X_test_preprocessed = preprocessing_pipeline.transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ddc4d",
   "metadata": {},
   "source": [
    "# Model training: Default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13529f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_final, y_train)\n",
    "\n",
    "# Support vector machine\n",
    "svm = SVR()\n",
    "svm.fit(X_train_final, y_train)\n",
    "\n",
    "# Random forest \n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train_final, y_train)\n",
    "\n",
    "# Neural network: Multi-layer perceptron \n",
    "mlp = MLPRegressor()\n",
    "mlp.fit(X_train_final, y_train)\n",
    "\n",
    "# XGBoost\n",
    "xgb = xgboost.XGBRegressor()\n",
    "xgb.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9fd4a4",
   "metadata": {},
   "source": [
    "# Model evaluation: Default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2280fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted values on validation set\n",
    "# Linear regression\n",
    "y_val_pred_reg = reg.predict(X_val_final)\n",
    "\n",
    "# Support vector machine\n",
    "y_val_pred_svm = svm.predict(X_val_final)\n",
    "\n",
    "# Random forest\n",
    "y_val_pred_rf = rf.predict(X_val_final)\n",
    "\n",
    "# Neural network\n",
    "y_val_pred_mlp = mlp.predict(X_val_final)\n",
    "\n",
    "# XGBoost\n",
    "y_val_pred_xgb = xgb.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb38239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics: RMSE, MAPE, R2\n",
    "# Linear regression\n",
    "reg_rmse = mean_squared_error(y_val, y_val_pred_reg, squared=False)\n",
    "reg_mape = mean_absolute_percentage_error(y_val, y_val_pred_reg)\n",
    "reg_r2 = r2_score(y_val, y_val_pred_reg)\n",
    "\n",
    "# Support vector machine \n",
    "svm_rmse = mean_squared_error(y_val, y_val_pred_svm, squared=False)\n",
    "svm_mape = mean_absolute_percentage_error(y_val, y_val_pred_svm)\n",
    "svm_r2 = r2_score(y_val, y_val_pred_svm)\n",
    "\n",
    "# Random forest\n",
    "rf_rmse = mean_squared_error(y_val, y_val_pred_rf, squared=False)\n",
    "rf_mape = mean_absolute_percentage_error(y_val, y_val_pred_rf)\n",
    "rf_r2 = r2_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Neural network \n",
    "mlp_rmse = mean_squared_error(y_val, y_val_pred_mlp, squared=False)\n",
    "mlp_mape = mean_absolute_percentage_error(y_val, y_val_pred_mlp)\n",
    "mlp_r2 = r2_score(y_val, y_val_pred_mlp)\n",
    "\n",
    "# XGBoost\n",
    "xgb_rmse = mean_squared_error(y_val, y_val_pred_xgb, squared=False)\n",
    "xgb_mape = mean_absolute_percentage_error(y_val, y_val_pred_xgb)\n",
    "xgb_r2 = r2_score(y_val, y_val_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table of model metrics\n",
    "# Create table \n",
    "comparison_table = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Support Vector Machine\", \"Random Forest\", \"Neural Network\", \"XGBoost\"],\n",
    "    \"RMSE\": [reg_rmse, svm_rmse, rf_rmse, mlp_rmse, xgb_rmse],\n",
    "    \"MAPE\": [reg_mape, svm_mape, rf_mape, mlp_mape, xgb_mape],\n",
    "    \"R-squared (RÂ²)\": [reg_r2, svm_r2, rf_r2, mlp_r2, xgb_r2]\n",
    "})\n",
    "\n",
    "# Show model comparison table\n",
    "print(round(comparison_table, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots to evaluate model performance\n",
    "# Create a dictionary of models with their predicted y values on the validation set\n",
    "model_dict = {\n",
    "    \"Linear Regression\": y_val_pred_reg, \n",
    "    \"Support Vector Machine\": y_val_pred_svm, \n",
    "    \"Random Forest\": y_val_pred_rf, \n",
    "    \"Neural Network\": y_val_pred_mlp,\n",
    "    \"XGBoost\": y_val_pred_xgb\n",
    "}\n",
    "\n",
    "# Iterate the models\n",
    "for model, y_val_pred in model_dict.items():\n",
    "    # Print model name\n",
    "    print(model)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = [actual_value - predicted_value for actual_value, predicted_value in zip(y_val, y_val_pred)]\n",
    "\n",
    "    # Create a 1x2 grid of subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=150)\n",
    "\n",
    "    # Plot 1: Predicted vs. Actual Prices\n",
    "    axes[0].scatter(y_val, y_val_pred)\n",
    "    axes[0].plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], color=\"red\", linestyle=\"--\", \n",
    "                 label=\"Perfect Prediction\")  # Add diagonal reference line\n",
    "    axes[0].set_xlabel(\"Actual Prices\")\n",
    "    axes[0].set_ylabel(\"Predicted Prices\")\n",
    "    axes[0].set_title(\"Predicted vs. Actual Prices\")\n",
    "    axes[0].grid(True)\n",
    "    axes[0].legend() \n",
    "\n",
    "    # Plot 2: Residuals vs. Actual Prices\n",
    "    axes[1].scatter(y_val, residuals)\n",
    "    axes[1].axhline(y=0, color=\"red\", linestyle=\"--\", label=\"Perfect Prediction\")  # Add horizontal reference line\n",
    "    axes[1].set_xlabel(\"Actual Prices\")\n",
    "    axes[1].set_ylabel(\"Residuals\")\n",
    "    axes[1].set_title(\"Residuals vs. Actual Prices\")\n",
    "    axes[1].grid(True)\n",
    "    axes[1].legend() \n",
    "\n",
    "    # Adjust layout and display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "775px",
    "left": "28px",
    "top": "191px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
